# -*- coding: utf-8 -*-
"""
å½“æ—¥ã‚¿ãƒ–ï¼ˆä¾‹: yyMMddï¼‰ã«ã‚ã‚‹URLï¼ˆCåˆ—ï¼‰ã‚’ã‚‚ã¨ã«ã€
Fåˆ—ä»¥é™ã«æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã€Påˆ—ã«ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€Qåˆ—ä»¥é™ã«ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’è¿½è¨˜ã™ã‚‹ã€‚

å‰æ:
- A:ã‚½ãƒ¼ã‚¹ / B:ã‚¿ã‚¤ãƒˆãƒ« / C:URL / D:æŠ•ç¨¿æ—¥ / E:æŽ²è¼‰å…ƒ ã¯æ—¢ã«å­˜åœ¨ï¼ˆmain.pyç­‰ã§ä½œæˆæ¸ˆã¿ï¼‰
- å½“æ—¥ã‚¿ãƒ–åã¯ JST ã® yyMMdd
- èªè¨¼ã¯ GOOGLE_CREDENTIALS(ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã®ä¸­èº«) ã¾ãŸã¯ credentials.json

ä»•æ§˜:
- æœ¬æ–‡ã¯æœ€å¤§10ãƒšãƒ¼ã‚¸åˆ†ã‚’ F..O åˆ—ã¸ (æœ¬æ–‡(1ãƒšãƒ¼ã‚¸) ï½ž æœ¬æ–‡(10ãƒšãƒ¼ã‚¸))
- ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’ P åˆ—ã¸
- ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’ Q åˆ—ä»¥é™ã«æ¨ªä¸¦ã³ã§æ ¼ç´ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆ1, ã‚³ãƒ¡ãƒ³ãƒˆ2, ...ï¼‰
"""

import os
import json
import time
from datetime import datetime, timezone, timedelta
from typing import List, Tuple

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# ===== è¨­å®š =====
SPREADSHEET_ID = "1UVwusLRcL4cZ3J9hnO6Z-f_d_sTFmocQJ9DcX3-v9u0"  # å‡ºåŠ›å…ˆã‚·ãƒ¼ãƒˆ
SHEET_NAME = datetime.now(timezone(timedelta(hours=9))).strftime("%y%m%d")  # å½“æ—¥ã‚¿ãƒ–
MAX_BODY_PAGES = 10
MAX_COMMENT_PAGES = 10
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}

# ===== èªè¨¼ =====
def build_gspread_client() -> gspread.Client:
    try:
        creds_str = os.environ.get("GOOGLE_CREDENTIALS")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
        else:
            with open("credentials.json", "r", encoding="utf-8") as f:
                info = json.load(f)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
        return gspread.authorize(credentials)
    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

# ===== ãƒ˜ãƒƒãƒ€ç®¡ç† =====
def ensure_sheet_and_headers(ws: gspread.Worksheet, max_comments: int) -> None:
    values = ws.get('A1:Z1')
    header = values[0] if values else []
    required = ["ã‚½ãƒ¼ã‚¹","ã‚¿ã‚¤ãƒˆãƒ«","URL","æŠ•ç¨¿æ—¥","æŽ²è¼‰å…ƒ"]
    body_headers = [f"æœ¬æ–‡({i}ãƒšãƒ¼ã‚¸)" for i in range(1, 11)]
    comments_count_header = ["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]
    comment_headers = [f"ã‚³ãƒ¡ãƒ³ãƒˆ{i}" for i in range(1, max(1, max_comments) + 1)]
    target_header = required + body_headers + comments_count_header + comment_headers
    if header != target_header:
        ws.update('A1', [target_header])

# ===== æœ¬æ–‡å–å¾— =====
def fetch_article_pages(base_url: str) -> Tuple[str, str, List[str]]:
    title = "å–å¾—ä¸å¯"
    article_date = "å–å¾—ä¸å¯"
    bodies: List[str] = []
    for page in range(1, MAX_BODY_PAGES + 1):
        url = base_url if page == 1 else f"{base_url}?page={page}"
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            res.raise_for_status()
        except Exception:
            break
        soup = BeautifulSoup(res.text, "html.parser")
        if page == 1:
            t = soup.find("title")
            if t and t.get_text(strip=True):
                title = t.get_text(strip=True).replace(" - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹", "")
            time_tag = soup.find("time")
            if time_tag:
                article_date = time_tag.get_text(strip=True)
        body_text = ""
        article = soup.find("article")
        if article:
            ps = article.find_all("p")
            body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
        if not body_text:
            main = soup.find("main")
            if main:
                ps = main.find_all("p")
                body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
        if not body_text or (bodies and body_text == bodies[-1]):
            break
        bodies.append(body_text)
    return title, article_date, bodies

# ===== ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— =====
def fetch_comments_with_selenium(base_url: str) -> List[str]:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2000")
    driver = webdriver.Chrome(options=options)  # âœ… Selenium Manager ãŒè‡ªå‹•è§£æ±º
    comments: List[str] = []
    try:
        for page in range(1, MAX_COMMENT_PAGES + 1):
            c_url = f"{base_url}/comments?page={page}"
            driver.get(c_url)
            time.sleep(2)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            p_candidates = []
            p_candidates.extend(soup.find_all("p", class_="sc-169yn8p-10"))
            p_candidates.extend(soup.select("p[data-ylk*='cm_body']"))
            p_candidates.extend(soup.select("p[class*='comment']"))
            page_comments = [p.get_text(strip=True) for p in p_candidates if p.get_text(strip=True)]
            if not page_comments:
                break
            if comments and page_comments and page_comments[0] == comments[-1]:
                break
            comments.extend(page_comments)
    finally:
        driver.quit()
    return comments

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def main():
    print(f"ðŸ“„ Spreadsheet: {SPREADSHEET_ID}")
    print(f"ðŸ“‘ Sheet: {SHEET_NAME}")
    gc = build_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SHEET_NAME)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=SHEET_NAME, rows="2000", cols="200")
    urls = ws.col_values(3)[1:]
    total = len(urls)
    print(f"ðŸ”Ž URLs to process: {total}")
    if total == 0:
        return
    rows_data: List[List[str]] = []
    max_comments = 0
    for idx, url in enumerate(urls, start=2):
        try:
            print(f"  - ({idx-1}/{total}) {url}")
            title, article_date, bodies = fetch_article_pages(url)
            comments = fetch_comments_with_selenium(url)
            body_cells = bodies[:MAX_BODY_PAGES] + [""] * (MAX_BODY_PAGES - len(bodies))
            comment_count = len(comments)
            row = body_cells + [comment_count] + comments
            rows_data.append(row)
            if comment_count > max_comments:
                max_comments = comment_count
        except Exception as e:
            print(f"    ! Error: {e}")
            row = ([""] * MAX_BODY_PAGES) + [0]
            rows_data.append(row)
    need_cols = MAX_BODY_PAGES + 1 + max_comments
    for i in range(len(rows_data)):
        if len(rows_data[i]) < need_cols:
            rows_data[i].extend([""] * (need_cols - len(rows_data[i])))
    ensure_sheet_and_headers(ws, max_comments=max_comments)
    ws.update("F2", rows_data)
    print(f"âœ… æ›¸ãè¾¼ã¿å®Œäº†: {len(rows_data)}è¡Œ / ã‚³ãƒ¡ãƒ³ãƒˆåˆ—={max_comments}")

if __name__ == "__main__":
    main()
