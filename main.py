# -*- coding: utf-8 -*-
"""
ãƒ•ãƒ­ãƒ¼ï¼ˆJSTï¼‰:
1) ã‚³ãƒ”ãƒ¼å…ƒã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ 'Yahoo'!A:D ã‹ã‚‰ã€Œå‰æ—¥15:00ã€œå½“æ—¥14:59:59ã€ã«å…¥ã‚‹è¡Œã ã‘å–å¾—
   A:ã‚¿ã‚¤ãƒˆãƒ« / B:URL / C:æŠ•ç¨¿æ—¥ / D:æ²è¼‰å…ƒ
2) å‡ºåŠ›å…ˆï¼ˆå½“æ—¥ã‚¿ãƒ– yyMMddï¼‰ã¸ Aã€œEåˆ—ã¨ã—ã¦è¿½è¨˜
   A:ã‚½ãƒ¼ã‚¹("Yahoo") / B:ã‚¿ã‚¤ãƒˆãƒ« / C:URL / D:æŠ•ç¨¿æ—¥(yy/m/d HH:MM) / E:æ²è¼‰å…ƒ
   - URLé‡è¤‡ã¯å½“æ—¥ã‚¿ãƒ–å†…ã§ã‚¹ã‚­ãƒƒãƒ—
3) ãã®å½“æ—¥ã‚¿ãƒ–ã® Cåˆ—URLã‚’èµ·ç‚¹ã«ã€æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã‚’ F..Oã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’ Pã€ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’ Q.. ã«è¿½è¨˜

èªè¨¼:
- GitHub Secrets: GOOGLE_CREDENTIALSï¼ˆã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã®â€œä¸­èº«â€ï¼‰
- å‡ºåŠ›å…ˆã¯å›ºå®š: 1UVwusLRcL4cZ3J9hnO6Z-f_d_sTFmocQJ9DcX3-v9u0

å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
- gspread, oauth2client, requests, beautifulsoup4, selenium (4.24+)
"""

import os
import json
import time
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Set

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# ====== è¨­å®š ======
# ã‚³ãƒ”ãƒ¼å…ƒï¼ˆYahooãƒªã‚¹ãƒˆï¼‰
SOURCE_SPREADSHEET_ID = "1RglATeTbLU1SqlfXnNToJqhXLdNoHCdePldioKDQgU8"
SOURCE_SHEET_NAME = "Yahoo"  # A:ã‚¿ã‚¤ãƒˆãƒ« / B:URL / C:æŠ•ç¨¿æ—¥ / D:æ²è¼‰å…ƒ

# å‡ºåŠ›å…ˆï¼ˆå›ºå®šãƒ»ã”æŒ‡å®šã®ã‚·ãƒ¼ãƒˆï¼‰
DEST_SPREADSHEET_ID = "1UVwusLRcL4cZ3J9hnO6Z-f_d_sTFmocQJ9DcX3-v9u0"

# æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—è¨­å®š
MAX_BODY_PAGES = 10  # æœ¬æ–‡ãƒšãƒ¼ã‚¸ä¸Šé™ã¯å¾“æ¥é€šã‚Šï¼ˆYahooæœ¬æ–‡ã¯å¤šãã¦ã‚‚æ•°ãƒšãƒ¼ã‚¸æƒ³å®šï¼‰
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}

# ã‚³ãƒ¡ãƒ³ãƒˆã®å®‰å…¨ä¸Šé™ï¼ˆç„¡é™ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—æš´èµ°é˜²æ­¢ï¼‰ã€‚å¿…è¦ãªã‚‰å¤‰æ›´å¯ã€‚
MAX_TOTAL_COMMENTS = 5000

TZ_JST = timezone(timedelta(hours=9))

# ====== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    """
    ã‚³ãƒ”ãƒ¼å…ƒCåˆ—ï¼ˆæŠ•ç¨¿æ—¥ï¼‰ã‚’ JST datetime ã«å¤‰æ›
    è¨±å®¹: "MM/DD HH:MM"ï¼ˆå¹´ã¯å½“å¹´è£œå®Œï¼‰, "YYYY/MM/DD HH:MM", "YYYY/MM/DD HH:MM:SS", Excelã‚·ãƒªã‚¢ãƒ«
    """
    if raw is None:
        return None
    if isinstance(raw, str):
        s = raw.strip()
        for fmt in ("%m/%d %H:%M", "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    dt = dt.replace(year=today_jst.year)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                pass
        return None
    if isinstance(raw, (int, float)):
        epoch = datetime(1899, 12, 30, tzinfo=TZ_JST)  # Excelèµ·ç‚¹
        return epoch + timedelta(days=float(raw))
    if isinstance(raw, datetime):
        return raw.astimezone(TZ_JST) if raw.tzinfo else raw.replace(tzinfo=TZ_JST)
    return None

def format_yy_m_d_hm(dt: datetime) -> str:
    """yy/m/d HH:MM ã«æ•´å½¢ï¼ˆå…ˆé ­ã‚¼ãƒ­ã®æœˆæ—¥ã‚’é¿ã‘ã‚‹ï¼‰"""
    yy = dt.strftime("%y")
    m = str(int(dt.strftime("%m")))
    d = str(int(dt.strftime("%d")))
    hm = dt.strftime("%H:%M")
    return f"{yy}/{m}/{d} {hm}"

# ====== èªè¨¼ ======
def build_gspread_client() -> gspread.Client:
    try:
        creds_str = os.environ.get("GOOGLE_CREDENTIALS")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open("credentials.json", "r", encoding="utf-8") as f:
                info = json.load(f)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
        return gspread.authorize(credentials)
    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

# ====== å‡ºåŠ›å…ˆã‚¿ãƒ–æ“ä½œ ======
def ensure_today_sheet(sh: gspread.Spreadsheet, today_tab: str) -> gspread.Worksheet:
    try:
        ws = sh.worksheet(today_tab)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=today_tab, rows="3000", cols="300")
    return ws

def get_existing_urls(ws: gspread.Worksheet) -> Set[str]:
    vals = ws.col_values(3)  # Cåˆ—=URL
    return set(vals[1:] if len(vals) > 1 else [])

def ensure_ae_header(ws: gspread.Worksheet) -> None:
    # A:ã‚½ãƒ¼ã‚¹ / B:ã‚¿ã‚¤ãƒˆãƒ« / C:URL / D:æŠ•ç¨¿æ—¥ / E:æ²è¼‰å…ƒ
    head = ws.row_values(1)
    target = ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "æ²è¼‰å…ƒ"]
    if head != target:
        ws.update('A1', [target])

def ensure_body_comment_headers(ws: gspread.Worksheet, max_comments: int) -> None:
    """
    1è¡Œç›®ã« F..O(æœ¬æ–‡1ã€œ10), P(ã‚³ãƒ¡ãƒ³ãƒˆæ•°), Q..(ã‚³ãƒ¡ãƒ³ãƒˆ1ã€œN) ã‚’æ•´ãˆã‚‹
    """
    current = ws.row_values(1)
    base = ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "æ²è¼‰å…ƒ"]
    body_headers = [f"æœ¬æ–‡({i}ãƒšãƒ¼ã‚¸)" for i in range(1, 11)]  # F..O
    comments_count = ["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]  # P
    comment_headers = [f"ã‚³ãƒ¡ãƒ³ãƒˆ{i}" for i in range(1, max(1, max_comments) + 1)]  # Q..
    target = base + body_headers + comments_count + comment_headers
    if current != target:
        ws.update('A1', [target])

# ====== ã‚³ãƒ”ãƒ¼å…ƒ â†’ å‡ºåŠ›å…ˆï¼ˆAã€œEåˆ—ï¼‰ ======
def transfer_a_to_e(gc: gspread.Client, dest_ws: gspread.Worksheet) -> int:
    sh_src = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    ws_src = sh_src.worksheet(SOURCE_SHEET_NAME)
    rows = ws_src.get('A:D')  # ãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€

    now = jst_now()
    start = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
    end = now.replace(hour=14, minute=59, second=59, microsecond=0)

    ensure_ae_header(dest_ws)
    existing = get_existing_urls(dest_ws)

    to_append: List[List[str]] = []
    for i, r in enumerate(rows):
        if i == 0:
            continue  # ãƒ˜ãƒƒãƒ€ãƒ¼
        title = r[0].strip() if len(r) > 0 and r[0] else ""
        url = r[1].strip() if len(r) > 1 and r[1] else ""
        posted_raw = r[2] if len(r) > 2 else ""
        site = r[3].strip() if len(r) > 3 and r[3] else ""
        if not title or not url:
            continue
        dt = parse_post_date(posted_raw, now)
        if not dt or not (start <= dt <= end):
            continue
        if url in existing:
            continue
        to_append.append(["Yahoo", title, url, format_yy_m_d_hm(dt), site])

    if to_append:
        # ã¾ã¨ã‚ã¦è¿½è¨˜ï¼ˆA:Eï¼‰
        dest_ws.append_rows(to_append, value_input_option="USER_ENTERED")
    return len(to_append)

# ====== æœ¬æ–‡å–å¾— ======
def fetch_article_pages(base_url: str) -> Tuple[str, str, List[str]]:
    title = "å–å¾—ä¸å¯"
    article_date = "å–å¾—ä¸å¯"
    bodies: List[str] = []
    for page in range(1, MAX_BODY_PAGES + 1):
        url = base_url if page == 1 else f"{base_url}?page={page}"
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            res.raise_for_status()
        except Exception:
            break
        soup = BeautifulSoup(res.text, "html.parser")
        if page == 1:
            t = soup.find("title")
            if t and t.get_text(strip=True):
                title = t.get_text(strip=True).replace(" - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹", "")
            time_tag = soup.find("time")
            if time_tag:
                article_date = time_tag.get_text(strip=True)

        body_text = ""
        article = soup.find("article")
        if article:
            ps = article.find_all("p")
            body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
        if not body_text:
            main = soup.find("main")
            if main:
                ps = main.find_all("p")
                body_text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))

        if not body_text or (bodies and body_text == bodies[-1]):
            break
        bodies.append(body_text)
    return title, article_date, bodies

# ====== ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ï¼ˆå…¨ãƒšãƒ¼ã‚¸ãƒ»å…¨ä»¶ï¼‰ ======
def fetch_comments_with_selenium(base_url: str) -> List[str]:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ /comments?page=1 ... ã§å…¨ãƒšãƒ¼ã‚¸å·¡å›ã—ã¦å…¨ä»¶å–å¾—ã€‚
    ç©ºãƒšãƒ¼ã‚¸ã«é”ã™ã‚‹ã‹ã€å‰å¾Œãƒšãƒ¼ã‚¸ã§é‡è¤‡ï¼ˆåŒä¸€å¢ƒç•Œï¼‰ã‚’æ¤œçŸ¥ã—ãŸã‚‰çµ‚äº†ã€‚
    æš´èµ°é˜²æ­¢ã« MAX_TOTAL_COMMENTS ã‚’è¶…ãˆãŸã‚‰æ‰“ã¡åˆ‡ã‚Šã€‚
    """
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2000")
    driver = webdriver.Chrome(options=options)  # Selenium Manager ãŒè‡ªå‹•ã§Driverè§£æ±º

    comments: List[str] = []
    last_tail: Optional[str] = None
    page = 1
    try:
        while True:
            c_url = f"{base_url}/comments?page={page}"
            driver.get(c_url)
            # è»½ã„å¾…æ©Ÿï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            time.sleep(2.0)

            soup = BeautifulSoup(driver.page_source, "html.parser")

            # ã§ãã‚‹ã ã‘å …ç‰¢ãªã‚»ãƒ¬ã‚¯ã‚¿ç¾¤ï¼ˆå°†æ¥å¤‰åŒ–ã«å‚™ãˆã¦è¤‡æ•°ä½µç”¨ï¼‰
            selectors = [
                "p.sc-169yn8p-10",                       # æ—¢å­˜
                "p[data-ylk*='cm_body']",                # æ—¢å­˜
                "p[class*='comment']",                    # æ—¢å­˜
                "div.commentBody, p.commentBody",        # æ±ç”¨
                "div[data-ylk*='cm_body']"               # æ±ç”¨
            ]

            p_candidates = []
            for sel in selectors:
                p_candidates.extend(soup.select(sel))

            page_comments = [p.get_text(strip=True) for p in p_candidates if p.get_text(strip=True)]
            # é‡è¤‡é™¤å»ï¼ˆåŒä¸€ãƒšãƒ¼ã‚¸å†…ã®é‡è¤‡ï¼‰
            page_comments = list(dict.fromkeys(page_comments))

            # ç©ºãƒšãƒ¼ã‚¸ãªã‚‰çµ‚äº†
            if not page_comments:
                break

            # å‰ãƒšãƒ¼ã‚¸æœ«å°¾ã¨ä»Šãƒšãƒ¼ã‚¸å…ˆé ­ãŒåŒã˜ãªã‚‰çµ‚äº†ï¼ˆå·¡å›çµ‚äº†ã®ã‚·ãƒ³ãƒ—ãƒ«åˆ¤å®šï¼‰
            if last_tail is not None and page_comments and page_comments[0] == last_tail:
                break

            # è“„ç©
            comments.extend(page_comments)

            # å®‰å…¨ä¸Šé™ã§æ‰“ã¡åˆ‡ã‚Š
            if len(comments) >= MAX_TOTAL_COMMENTS:
                comments = comments[:MAX_TOTAL_COMMENTS]
                break

            # æ¬¡ãƒšãƒ¼ã‚¸ã¸
            last_tail = page_comments[-1]
            page += 1

    finally:
        driver.quit()

    return comments

# ====== æœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãè¾¼ã¿ ======
def write_bodies_and_comments(ws: gspread.Worksheet) -> None:
    urls = ws.col_values(3)[1:]  # Cåˆ—URLï¼ˆ2è¡Œç›®ä»¥é™ï¼‰
    total = len(urls)
    print(f"ğŸ” URLs to process: {total}")
    if total == 0:
        return

    rows_data: List[List[str]] = []
    max_comments = 0
    for idx, url in enumerate(urls, start=2):
        try:
            print(f"  - ({idx-1}/{total}) {url}")
            _title, _date, bodies = fetch_article_pages(url)
            comments = fetch_comments_with_selenium(url)

            body_cells = bodies[:MAX_BODY_PAGES] + [""] * (MAX_BODY_PAGES - len(bodies))
            cnt = len(comments)
            row = body_cells + [cnt] + comments
            rows_data.append(row)
            if cnt > max_comments:
                max_comments = cnt
        except Exception as e:
            print(f"    ! Error: {e}")
            rows_data.append(([""] * MAX_BODY_PAGES) + [0])

    # åˆ—å¹…ã‚’æœ€å¤§ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã«
    need_cols = MAX_BODY_PAGES + 1 + max_comments
    for i in range(len(rows_data)):
        if len(rows_data[i]) < need_cols:
            rows_data[i].extend([""] * (need_cols - len(rows_data[i])))

    # ãƒ˜ãƒƒãƒ€ãƒ¼æ•´å‚™ï¼ˆæœ¬æ–‡ã¨ã‚³ãƒ¡ãƒ³ãƒˆã®åˆ—ã‚’å«ã‚€å®Œå…¨ç‰ˆï¼‰
    ensure_body_comment_headers(ws, max_comments=max_comments)

    # F2 ã‹ã‚‰ä¸€æ‹¬æ›´æ–°
    if rows_data:
        ws.update("F2", rows_data)
        print(f"âœ… æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãè¾¼ã¿: {len(rows_data)} è¡Œï¼ˆæœ€å¤§ã‚³ãƒ¡ãƒ³ãƒˆåˆ—={max_comments}ï¼‰")

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print(f"ğŸ“„ DEST Spreadsheet: {DEST_SPREADSHEET_ID}")
    gc = build_gspread_client()
    dest_sh = gc.open_by_key(DEST_SPREADSHEET_ID)
    today_tab = jst_now().strftime("%y%m%d")
    ws = ensure_today_sheet(dest_sh, today_tab)

    # 1) Aã€œE ã‚’åŸ‹ã‚ã‚‹
    added = transfer_a_to_e(gc, ws)
    print(f"ğŸ“ Aã€œE è¿½è¨˜: {added} è¡Œ")

    # 2) Fä»¥é™ï¼ˆæœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆï¼‰ã‚’åŸ‹ã‚ã‚‹
    write_bodies_and_comments(ws)

if __name__ == "__main__":
    main()
